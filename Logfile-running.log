/media/res/HDD_AB/projekte/kd0241-py/Q-KAT/AAUNaL/Authorship-attribution-using-ngram-and-LLMs-main$ python3 classifier-R3.py authorlist -approach discriminative
Discriminative
Map: 100%|███████████████████████████████████████████████████| 18910/18910 [00:05<00:00, 3471.88 examples/s]
Map: 100%|█████████████████████████████████████████████████████| 2365/2365 [00:01<00:00, 2168.83 examples/s]
Map: 100%|█████████████████████████████████████████████████████| 2363/2363 [00:00<00:00, 3005.21 examples/s]
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'loss': 0.7215, 'grad_norm': 8.503549575805664, 'learning_rate': 1.1539763113367176e-05, 'epoch': 0.85}
{'eval_loss': 0.4688766598701477, 'eval_accuracy': 0.8205670757511637, 'eval_runtime': 227.3232, 'eval_samples_per_second': 10.395, 'eval_steps_per_second': 0.326, 'epoch': 1.0}
{'loss': 0.4151, 'grad_norm': 5.4659295082092285, 'learning_rate': 3.079526226734349e-06, 'epoch': 1.69}
{'eval_loss': 0.43643736839294434, 'eval_accuracy': 0.8311468472280998, 'eval_runtime': 229.0729, 'eval_samples_per_second': 10.315, 'eval_steps_per_second': 0.323, 'epoch': 2.0}
{'train_runtime': 12142.1289, 'train_samples_per_second': 3.115, 'train_steps_per_second': 0.097, 'train_loss': 0.5375427271912344, 'epoch': 2.0}
100%|█████████████████████████████████████████████████████████████████| 1182/1182 [3:22:22<00:00, 10.27s/it]
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Austen Accuracy 31.656184486373167
Wilde Accuracy 76.44710578842316
Dickens Accuracy 0.0
Tolstoy Accuracy 0.0

