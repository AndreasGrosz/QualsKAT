python3 main.py --train                                         (aaunal)  
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer
'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes
for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understan
d what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggin
gface/transformers/pull/24565 
/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: 
FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavio
r will be depracted in transformers v4.45, and will be then set to `False` by default. For more details chec
k this issue: https://github.com/huggingface/transformers/issues/31884 
 warnings.warn( 
Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at t5-large and a
re newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification
_head.out_proj.bias', 'classification_head.out_proj.weight'] 
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inferenc
e. 
Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allena
i/longformer-base-4096 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'clas
sifier.out_proj.bias', 'classifier.out_proj.weight'] 
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inferenc
e. 
Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large
-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.bias'
, 'sequence_summary.summary.weight'] 
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inferenc
e. 
Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/el
ectra-large-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'c
lassifier.out_proj.bias', 'classifier.out_proj.weight'] 
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inferenc
e. 
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /home/
res/projekte/kd0241-py/Q-KAT/AAUNaL/fresh-models/distilbert-base-uncased and are newly initialized because t
he shapes did not match: 
- classifier.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([53]) in the model instantia
ted 
- classifier.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([53, 768]) in the mod
el instantiated 
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inferenc
e. 
Map: 100%|██████████████████████████████████████████████████████| 2428/2428 [00:04<00:00, 575.68 examples/s] 
/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarn
ing: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 珞 Transformers. Use `eval_s
trategy` instead 
 warnings.warn( 
 0%|                                                                               | 0/318 [00:00<?, ?it/s]
/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: 
torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will rais
e an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preser
ve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the diff
erences between the two variants. 
 return fn(*args, **kwargs) 
/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: 
`torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead. 
 with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type:
ignore[attr-defined] 
{'loss': 1.7747, 'grad_norm': 4.9178972244262695, 'learning_rate': 1.685534591194969e-05, 'epoch': 0.47}     
{'loss': 0.2941, 'grad_norm': 1.381597638130188, 'learning_rate': 1.3773584905660378e-05, 'epoch': 0.94}     
{'eval_loss': 0.09309395402669907, 'eval_runtime': 1.7223, 'eval_samples_per_second': 211.345, 'eval_steps_p
er_second': 105.672, 'epoch': 0.94}                                                                          
31%|█████████████████████▋                                               | 100/318 [00:38<01:18,  2.77it/s/
home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: t
orch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise
an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserv
e the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the diffe
rences between the two variants. 
 return fn(*args, **kwargs) 
/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: 
`torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead. 
 with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type:
ignore[attr-defined] 
{'loss': 0.0545, 'grad_norm': 0.581115186214447, 'learning_rate': 1.069182389937107e-05, 'epoch': 1.41}      
{'loss': 0.0271, 'grad_norm': 0.22737808525562286, 'learning_rate': 7.5471698113207555e-06, 'epoch': 1.88}   
{'eval_loss': 0.031068645417690277, 'eval_runtime': 1.7259, 'eval_samples_per_second': 210.901, 'eval_steps_
per_second': 105.451, 'epoch': 1.88}                                                                         
63%|███████████████████████████████████████████▍                         | 200/318 [01:16<00:42,  2.76it/s/
home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: t
orch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise
an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserv
e the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the diffe
rences between the two variants. 
 return fn(*args, **kwargs) 
/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: 
`torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead. 
 with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type:
ignore[attr-defined] 
{'loss': 0.0142, 'grad_norm': 0.12689462304115295, 'learning_rate': 4.402515723270441e-06, 'epoch': 2.35}    
{'loss': 0.0095, 'grad_norm': 0.1426868885755539, 'learning_rate': 1.257861635220126e-06, 'epoch': 2.82}     
{'eval_loss': 0.028896374627947807, 'eval_runtime': 1.7252, 'eval_samples_per_second': 210.993, 'eval_steps_
per_second': 105.497, 'epoch': 2.82}                                                                         
94%|█████████████████████████████████████████████████████████████████    | 300/318 [01:55<00:06,  2.76it/s/
home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: t
orch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise
an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserv
e the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the diffe
rences between the two variants. 
 return fn(*args, **kwargs) 
/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: 
`torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead. 
 with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type:
ignore[attr-defined] 
{'train_runtime': 122.887, 'train_samples_per_second': 41.477, 'train_steps_per_second': 2.588, 'train_loss'
: 0.3434952812374763, 'epoch': 2.99} 
100%|█████████████████████████████████████████████████████████████████████| 318/318 [02:02<00:00,  2.59it/s] 
100%|████████████████████████████████████████████████████████████████████| 183/183 [00:01<00:00, 104.43it/s] 
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /home/res
/projekte/kd0241-py/Q-KAT/AAUNaL/fresh-models/roberta-large and are newly initialized because the shapes did
not match: 
- classifier.out_proj.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([53]) in the model 
instantiated 
- classifier.out_proj.weight: found shape torch.Size([2, 1024]) in the checkpoint and torch.Size([53, 1024])
in the model instantiated 
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inferenc
e. 
Map: 100%|██████████████████████████████████████████████████████| 2428/2428 [00:06<00:00, 399.86 examples/s] 
/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarn
ing: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 珞 Transformers. Use `eval_s
trategy` instead 
 warnings.warn( 
 0%|                                                                               | 0/318 [00:00<?, ?it/s]
/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: 
torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will rais
e an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preser
ve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the diff
erences between the two variants. 
 return fn(*args, **kwargs) 
/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: 
`torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead. 
 with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type:
ignore[attr-defined] 
{'loss': 0.6699, 'grad_norm': 2.410861015319824, 'learning_rate': 1.69811320754717e-05, 'epoch': 0.47}       
{'loss': 0.0207, 'grad_norm': 0.016760004684329033, 'learning_rate': 1.389937106918239e-05, 'epoch': 0.94}   
{'eval_loss': 0.00866665318608284, 'eval_runtime': 11.2183, 'eval_samples_per_second': 32.447, 'eval_steps_p
er_second': 16.223, 'epoch': 0.94}                                                                           
31%|█████████████████████▋                                               | 100/318 [03:56<08:10,  2.25s/it/
home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: t
orch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise
an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserv
e the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the diffe
rences between the two variants. 
 return fn(*args, **kwargs) 
/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: 
`torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead. 
 with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type:
ignore[attr-defined] 
{'loss': 0.0045, 'grad_norm': 30.97693634033203, 'learning_rate': 1.0754716981132076e-05, 'epoch': 1.41}     
{'loss': 0.001, 'grad_norm': 0.008286034688353539, 'learning_rate': 7.6100628930817626e-06, 'epoch': 1.88}   
{'eval_loss': 0.0019242992857471108, 'eval_runtime': 11.2198, 'eval_samples_per_second': 32.443, 'eval_steps
_per_second': 16.221, 'epoch': 1.88}                                                                         
63%|███████████████████████████████████████████▍                         | 200/318 [07:56<04:26,  2.26s/it/
home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: t
orch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise
an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserv
e the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the diffe
rences between the two variants. 
 return fn(*args, **kwargs) 
/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: 
`torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead. 
 with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type:
ignore[attr-defined] 
{'loss': 0.0005, 'grad_norm': 0.007148833479732275, 'learning_rate': 4.465408805031447e-06, 'epoch': 2.35}   
{'loss': 0.0004, 'grad_norm': 0.006738084834069014, 'learning_rate': 1.3207547169811322e-06, 'epoch': 2.82}  
{'eval_loss': 0.008300197310745716, 'eval_runtime': 11.2186, 'eval_samples_per_second': 32.446, 'eval_steps_
per_second': 16.223, 'epoch': 2.82}                                                                          
94%|█████████████████████████████████████████████████████████████████    | 300/318 [11:56<00:40,  2.26s/it/
home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: t
orch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise
an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserv
e the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the diffe
rences between the two variants. 
 return fn(*args, **kwargs) 
/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: 
`torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead. 
 with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type:
ignore[attr-defined] 
{'train_runtime': 763.8253, 'train_samples_per_second': 6.673, 'train_steps_per_second': 0.416, 'train_loss'
: 0.10960535381762486, 'epoch': 2.99} 
100%|█████████████████████████████████████████████████████████████████████| 318/318 [12:43<00:00,  2.40s/it] 
100%|█████████████████████████████████████████████████████████████████████| 183/183 [00:11<00:00, 16.28it/s] 
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /home/res
/projekte/kd0241-py/Q-KAT/AAUNaL/fresh-models/roberta-base and are newly initialized because the shapes did 
not match: 
- classifier.out_proj.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([53]) in the model 
instantiated 
- classifier.out_proj.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([53, 768]) i
n the model instantiated 
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inferenc
e. 
Map: 100%|██████████████████████████████████████████████████████| 2428/2428 [00:05<00:00, 410.50 examples/s] 
/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarn
ing: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 珞 Transformers. Use `eval_s
trategy` instead 
 warnings.warn( 
 0%|                                                                               | 0/318 [00:00<?, ?it/s]
/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: 
torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will rais
e an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preser
ve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the diff
erences between the two variants. 
 return fn(*args, **kwargs) 
/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: 
`torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead. 
 with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type:
ignore[attr-defined] 
{'loss': 1.3451, 'grad_norm': 4.528257369995117, 'learning_rate': 1.6918238993710692e-05, 'epoch': 0.47}     
{'loss': 0.0604, 'grad_norm': 0.2003777176141739, 'learning_rate': 1.3773584905660378e-05, 'epoch': 0.94}    
{'eval_loss': 0.020122412592172623, 'eval_runtime': 3.6039, 'eval_samples_per_second': 101.002, 'eval_steps_
per_second': 50.501, 'epoch': 0.94}                                                                          
31%|█████████████████████▋                                               | 100/318 [01:18<02:42,  1.34it/s/
home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: t
orch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise
an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserv
e the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the diffe
rences between the two variants. 
 return fn(*args, **kwargs) 
/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: 
`torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead. 
 with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type:
ignore[attr-defined] 
{'loss': 0.0161, 'grad_norm': 0.5684492588043213, 'learning_rate': 1.069182389937107e-05, 'epoch': 1.41}     
{'loss': 0.0059, 'grad_norm': 0.08055607974529266, 'learning_rate': 7.5471698113207555e-06, 'epoch': 1.88}   
{'eval_loss': 0.003711202647536993, 'eval_runtime': 3.5994, 'eval_samples_per_second': 101.128, 'eval_steps_
per_second': 50.564, 'epoch': 1.88}                                                                          
63%|███████████████████████████████████████████▍                         | 200/318 [02:37<01:27,  1.34it/s/
home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: t
orch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise
an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserv
e the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the diffe
rences between the two variants. 
 return fn(*args, **kwargs) 
/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: 
`torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead. 
 with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type:
ignore[attr-defined] 
{'loss': 0.0058, 'grad_norm': 0.05654746666550636, 'learning_rate': 4.402515723270441e-06, 'epoch': 2.35}    
{'loss': 0.0065, 'grad_norm': 0.04921308532357216, 'learning_rate': 1.257861635220126e-06, 'epoch': 2.82}    
{'eval_loss': 0.0026144850999116898, 'eval_runtime': 3.5995, 'eval_samples_per_second': 101.126, 'eval_steps
_per_second': 50.563, 'epoch': 2.82}                                                                         
94%|█████████████████████████████████████████████████████████████████    | 300/318 [03:56<00:13,  1.34it/s/
home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: t
orch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise
an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserv
e the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the diffe
rences between the two variants. 
 return fn(*args, **kwargs) 
/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: 
`torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead. 
 with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type:
ignore[attr-defined] 
{'train_runtime': 251.8986, 'train_samples_per_second': 20.234, 'train_steps_per_second': 1.262, 'train_loss
': 0.22658227224769834, 'epoch': 2.99} 
100%|█████████████████████████████████████████████████████████████████████| 318/318 [04:11<00:00,  1.26it/s] 
100%|█████████████████████████████████████████████████████████████████████| 183/183 [00:03<00:00, 50.73it/s] 
Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at /home/res
/projekte/kd0241-py/Q-KAT/AAUNaL/fresh-models/microsoft/deberta-base and are newly initialized because the s
hapes did not match: 
- classifier.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([53]) in the model instantia
ted 
- classifier.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([53, 768]) in the mod
el instantiated 
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inferenc
e. 
Map: 100%|██████████████████████████████████████████████████████| 2428/2428 [00:06<00:00, 397.70 examples/s] 
/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarn
ing: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 珞 Transformers. Use `eval_s
trategy` instead 
 warnings.warn( 
 0%|                                                                               | 0/318 [00:00<?, ?it/s]
/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: 
torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will rais
e an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preser
ve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the diff
erences between the two variants. 
 return fn(*args, **kwargs) 
/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: 
`torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead. 
 with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type:
ignore[attr-defined] 
{'loss': 1.2001, 'grad_norm': 0.879882276058197, 'learning_rate': 1.6918238993710692e-05, 'epoch': 0.47}     
{'loss': 0.0315, 'grad_norm': 0.1396484673023224, 'learning_rate': 1.3773584905660378e-05, 'epoch': 0.94}    
{'eval_loss': 0.05542498826980591, 'eval_runtime': 6.2304, 'eval_samples_per_second': 58.423, 'eval_steps_pe
r_second': 29.211, 'epoch': 0.94}                                                                            
31%|█████████████████████▋                                               | 100/318 [02:20<04:52,  1.34s/it/
home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: t
orch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise
an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserv
e the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the diffe
rences between the two variants. 
 return fn(*args, **kwargs) 
/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: 
`torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead. 
 with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type:
ignore[attr-defined] 
{'loss': 0.0109, 'grad_norm': 0.3445761501789093, 'learning_rate': 1.069182389937107e-05, 'epoch': 1.41}     
{'loss': 0.0031, 'grad_norm': 0.05828607454895973, 'learning_rate': 7.5471698113207555e-06, 'epoch': 1.88}   
{'eval_loss': 0.01811794377863407, 'eval_runtime': 6.2209, 'eval_samples_per_second': 58.513, 'eval_steps_pe
r_second': 29.256, 'epoch': 1.88}                                                                            
63%|███████████████████████████████████████████▍                         | 200/318 [04:41<02:38,  1.34s/it/
home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: t
orch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise
an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserv
e the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the diffe
rences between the two variants. 
 return fn(*args, **kwargs) 
/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: 
`torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead. 
 with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type:
ignore[attr-defined] 
{'loss': 0.0025, 'grad_norm': 0.033643171191215515, 'learning_rate': 4.402515723270441e-06, 'epoch': 2.35}   
{'loss': 0.0021, 'grad_norm': 0.03526376560330391, 'learning_rate': 1.257861635220126e-06, 'epoch': 2.82}    
{'eval_loss': 0.015819737687706947, 'eval_runtime': 6.2265, 'eval_samples_per_second': 58.459, 'eval_steps_p
er_second': 29.23, 'epoch': 2.82}                                                                            
94%|█████████████████████████████████████████████████████████████████    | 300/318 [07:03<00:24,  1.34s/it/
home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: t
orch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise
an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserv
e the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the diffe
rences between the two variants. 
 return fn(*args, **kwargs) 
/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: 
`torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead. 
 with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type:
ignore[attr-defined] 
{'train_runtime': 450.3396, 'train_samples_per_second': 11.318, 'train_steps_per_second': 0.706, 'train_loss
': 0.19666546717895278, 'epoch': 2.99} 
100%|█████████████████████████████████████████████████████████████████████| 318/318 [07:30<00:00,  1.42s/it] 
100%|█████████████████████████████████████████████████████████████████████| 183/183 [00:06<00:00, 29.29it/s] 
Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at /home/res/
projekte/kd0241-py/Q-KAT/AAUNaL/fresh-models/albert-base-v2 and are newly initialized because the shapes did
not match: 
- classifier.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([53]) in the model instantia
ted 
- classifier.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([53, 768]) in the mod
el instantiated 
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inferenc
e. 
Map: 100%|██████████████████████████████████████████████████████| 2428/2428 [00:05<00:00, 442.07 examples/s] 
/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarn
ing: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 珞 Transformers. Use `eval_s
trategy` instead 
 warnings.warn( 
{'loss': 0.8418, 'grad_norm': 3.4617228507995605, 'learning_rate': 1.69811320754717e-05, 'epoch': 0.47}      
{'loss': 0.0948, 'grad_norm': 0.15484893321990967, 'learning_rate': 1.389937106918239e-05, 'epoch': 0.94}    
{'eval_loss': 0.005407511722296476, 'eval_runtime': 3.8653, 'eval_samples_per_second': 94.171, 'eval_steps_p
er_second': 47.086, 'epoch': 0.94}                                                                           
{'loss': 0.0202, 'grad_norm': 106.27050018310547, 'learning_rate': 1.0754716981132076e-05, 'epoch': 1.41}    
{'loss': 0.0191, 'grad_norm': 0.04699929431080818, 'learning_rate': 7.6100628930817626e-06, 'epoch': 1.88}   
{'eval_loss': 0.001997151179239154, 'eval_runtime': 3.8661, 'eval_samples_per_second': 94.152, 'eval_steps_p
er_second': 47.076, 'epoch': 1.88}                                                                           
{'loss': 0.009, 'grad_norm': 0.03156272694468498, 'learning_rate': 4.465408805031447e-06, 'epoch': 2.35}     
{'loss': 0.0027, 'grad_norm': 0.02842099219560623, 'learning_rate': 1.3207547169811322e-06, 'epoch': 2.82}   
{'eval_loss': 0.0015136462170630693, 'eval_runtime': 3.8667, 'eval_samples_per_second': 94.138, 'eval_steps_
per_second': 47.069, 'epoch': 2.82}                                                                          
{'train_runtime': 183.0912, 'train_samples_per_second': 27.839, 'train_steps_per_second': 1.737, 'train_loss
': 0.1558050569498314, 'epoch': 2.99}                                                                        
100%|█████████████████████████████████████████████████████████████████████| 318/318 [03:03<00:00,  1.74it/s] 
100%|█████████████████████████████████████████████████████████████████████| 183/183 [00:03<00:00, 47.25it/s] 
/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: 
FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavio
r will be depracted in transformers v4.45, and will be then set to `False` by default. For more details chec
k this issue: https://github.com/huggingface/transformers/issues/31884 
 warnings.warn( 
Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at t5-large and a
re newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification
_head.out_proj.bias', 'classification_head.out_proj.weight'] 
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inferenc
e. 
Map: 100%|██████████████████████████████████████████████████████| 2428/2428 [00:05<00:00, 426.06 examples/s] 
/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarn
ing: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 珞 Transformers. Use `eval_s
trategy` instead 
 warnings.warn( 
 0%|                                                                               | 0/318 [00:00<?, ?it/s]
/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: 
torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will rais
e an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preser
ve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the diff
erences between the two variants. 
 return fn(*args, **kwargs) 
/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: 
`torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead. 
 with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type:
ignore[attr-defined] 
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 2e-05, 'epoch': 0.47}                                       
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 2e-05, 'epoch': 0.94}                                       
{'eval_loss': nan, 'eval_runtime': 33.8304, 'eval_samples_per_second': 10.76, 'eval_steps_per_second': 5.38,
'epoch': 0.94}                                                                                              
31%|█████████████████████▋                                               | 100/318 [11:00<22:45,  6.26s/it/
home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: t
orch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise
an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserv
e the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the diffe
rences between the two variants. 
 return fn(*args, **kwargs) 
/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: 
`torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead. 
 with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type:
ignore[attr-defined] 
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 2e-05, 'epoch': 1.41}                                       
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 2e-05, 'epoch': 1.88}                                       
{'eval_loss': nan, 'eval_runtime': 33.8285, 'eval_samples_per_second': 10.76, 'eval_steps_per_second': 5.38,
'epoch': 1.88}                                                                                              
63%|███████████████████████████████████████████▍                         | 200/318 [22:02<12:18,  6.26s/it/
home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: t
orch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise
an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserv
e the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the diffe
rences between the two variants. 
 return fn(*args, **kwargs) 
/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: 
`torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead. 
 with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type:
ignore[attr-defined] 
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 2e-05, 'epoch': 2.35}                                       
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 2e-05, 'epoch': 2.82}                                       
{'eval_loss': nan, 'eval_runtime': 33.8229, 'eval_samples_per_second': 10.762, 'eval_steps_per_second': 5.38
1, 'epoch': 2.82}                                                                                            
94%|█████████████████████████████████████████████████████████████████    | 300/318 [33:04<01:52,  6.26s/it/
home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: t
orch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise
an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserv
e the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the diffe
rences between the two variants. 
 return fn(*args, **kwargs) 
/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: 
`torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead. 
 with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type:
ignore[attr-defined] 
100%|█████████████████████████████████████████████████████████████████████| 318/318 [34:59<00:00,  6.29s/it]
There were missing keys in the checkpoint model loaded: ['transformer.encoder.embed_tokens.weight', 'transfo
rmer.decoder.embed_tokens.weight']. 
{'train_runtime': 2102.3965, 'train_samples_per_second': 2.424, 'train_steps_per_second': 0.151, 'train_loss
': 0.0, 'epoch': 2.99} 
100%|█████████████████████████████████████████████████████████████████████| 318/318 [35:02<00:00,  6.61s/it] 
100%|█████████████████████████████████████████████████████████████████████| 183/183 [00:33<00:00,  5.43it/s] 
/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: 
FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavio
r will be depracted in transformers v4.45, and will be then set to `False` by default. For more details chec
k this issue: https://github.com/huggingface/transformers/issues/31884 
 warnings.warn( 
Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allena
i/longformer-base-4096 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'clas
sifier.out_proj.bias', 'classifier.out_proj.weight'] 
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inferenc
e. 
Map: 100%|██████████████████████████████████████████████████████| 2428/2428 [00:05<00:00, 408.04 examples/s] 
/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarn
ing: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 珞 Transformers. Use `eval_s
trategy` instead 
 warnings.warn( 
 0%|                                                                               | 0/318 [00:00<?, ?it/s]
Initializing global attention on CLS token... 
/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: 
torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will rais
e an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preser
ve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the diff
erences between the two variants. 
 return fn(*args, **kwargs) 
/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: 
`torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead. 
 with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type:
ignore[attr-defined] 
{'loss': 1.0739, 'grad_norm': 2.498746633529663, 'learning_rate': 1.6918238993710692e-05, 'epoch': 0.47}     
{'loss': 0.0373, 'grad_norm': 0.1694934368133545, 'learning_rate': 1.3773584905660378e-05, 'epoch': 0.94}    
{'eval_loss': 0.02512819692492485, 'eval_runtime': 14.2526, 'eval_samples_per_second': 25.539, 'eval_steps_p
er_second': 12.77, 'epoch': 0.94}                                                                            
31%|█████████████████████▋                                               | 100/318 [03:22<06:49,  1.88s/it/
home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: t
orch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise
an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserv
e the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the diffe
rences between the two variants. 
 return fn(*args, **kwargs) 
/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: 
`torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead. 
 with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type:
ignore[attr-defined] 
{'loss': 0.0223, 'grad_norm': inf, 'learning_rate': 1.069182389937107e-05, 'epoch': 1.41}                    
{'loss': 0.006, 'grad_norm': 0.07511702179908752, 'learning_rate': 7.5471698113207555e-06, 'epoch': 1.88}    
{'eval_loss': 0.003595493733882904, 'eval_runtime': 14.2719, 'eval_samples_per_second': 25.505, 'eval_steps_
per_second': 12.752, 'epoch': 1.88}                                                                          
63%|███████████████████████████████████████████▍                         | 200/318 [06:45<03:42,  1.88s/it/
home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: t
orch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise
an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserv
e the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the diffe
rences between the two variants. 
 return fn(*args, **kwargs) 
/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: 
`torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead. 
 with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type:
ignore[attr-defined] 
{'loss': 0.0049, 'grad_norm': 0.05907214805483818, 'learning_rate': 4.402515723270441e-06, 'epoch': 2.35}    
{'loss': 0.0047, 'grad_norm': 0.05611147731542587, 'learning_rate': 1.257861635220126e-06, 'epoch': 2.82}    
{'eval_loss': 0.002804666990414262, 'eval_runtime': 14.2712, 'eval_samples_per_second': 25.506, 'eval_steps_
per_second': 12.753, 'epoch': 2.82}                                                                          
94%|█████████████████████████████████████████████████████████████████    | 300/318 [10:09<00:33,  1.88s/it/
home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: t
orch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise
an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserv
e the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the diffe
rences between the two variants. 
 return fn(*args, **kwargs) 
/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: 
`torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead. 
 with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type:
ignore[attr-defined] 
{'train_runtime': 646.1842, 'train_samples_per_second': 7.888, 'train_steps_per_second': 0.492, 'train_loss'
: 0.1809032938015536, 'epoch': 2.99} 
100%|█████████████████████████████████████████████████████████████████████| 318/318 [10:46<00:00,  2.03s/it] 
92%|███████████████████████████████████████████████████████████████▋     | 169/183 [00:13<00:01, 12.60it/s] 
100%|█████████████████████████████████████████████████████████████████████| 183/183 [00:14<00:00, 12.71it/s] 
/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: 
FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavio
r will be depracted in transformers v4.45, and will be then set to `False` by default. For more details chec
k this issue: https://github.com/huggingface/transformers/issues/31884 
 warnings.warn( 
Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large
-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.bias'
, 'sequence_summary.summary.weight'] 
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inferenc
e. 
Map: 100%|██████████████████████████████████████████████████████| 2428/2428 [00:05<00:00, 416.06 examples/s] 
/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarn
ing: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 珞 Transformers. Use `eval_s
trategy` instead 
 warnings.warn( 
 1%|▍                                                                      | 2/318 [00:05<13:14,  2.51s/it]
Traceback (most recent call last): 
 File "/home/res/projekte/kd0241-py/Q-KAT/AAUNaL/main.py", line 215, in <module> 
   main() 
 File "/home/res/projekte/kd0241-py/Q-KAT/AAUNaL/main.py", line 114, in main 
   trainer.train() 
 File "/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/transformers/trainer.py", line 1948, i
n train 
   return inner_training_loop( 
 File "/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/transformers/trainer.py", line 2289, i
n _inner_training_loop 
   tr_loss_step = self.training_step(model, inputs) 
 File "/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/transformers/trainer.py", line 3328, i
n training_step 
   loss = self.compute_loss(model, inputs) 
 File "/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/transformers/trainer.py", line 3373, i
n compute_loss 
   outputs = model(**inputs) 
 File "/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553
, in _wrapped_call_impl 
   return self._call_impl(*args, **kwargs) 
 File "/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562
, in _call_impl 
   return forward_call(*args, **kwargs) 
 File "/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/accelerate/utils/operations.py", line 
819, in forward 
   return model_forward(*args, **kwargs) 
 File "/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/accelerate/utils/operations.py", line 
807, in __call__ 
   return convert_to_fp32(self.model_forward(*args, **kwargs)) 
 File "/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/amp/autocast_mode.py", line 43, 
in decorate_autocast 
   return func(*args, **kwargs) 
 File "/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xln
et.py", line 1540, in forward 
   transformer_outputs = self.transformer( 
 File "/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553
, in _wrapped_call_impl 
   return self._call_impl(*args, **kwargs) 
 File "/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562
, in _call_impl 
   return forward_call(*args, **kwargs) 
 File "/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xln
et.py", line 1232, in forward 
   outputs = layer_module( 
 File "/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553
, in _wrapped_call_impl 
   return self._call_impl(*args, **kwargs) 
 File "/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562
, in _call_impl 
   return forward_call(*args, **kwargs) 
 File "/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xln
et.py", line 503, in forward 
   outputs = self.rel_attn( 
 File "/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553
, in _wrapped_call_impl 
   return self._call_impl(*args, **kwargs) 
 File "/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562
, in _call_impl 
   return forward_call(*args, **kwargs) 
 File "/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xln
et.py", line 434, in forward 
   attn_vec = self.rel_attn_core( 
 File "/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xln
et.py", line 296, in rel_attn_core 
   attn_prob = self.dropout(attn_prob) 
 File "/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553
, in _wrapped_call_impl 
   return self._call_impl(*args, **kwargs) 
 File "/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562
, in _call_impl 
   return forward_call(*args, **kwargs) 
 File "/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/nn/modules/dropout.py", line 59,
in forward 
   return F.dropout(input, self.p, self.training, self.inplace) 
 File "/home/res/miniconda3/envs/aaunal/lib/python3.10/site-packages/torch/nn/functional.py", line 1295, in
dropout 
   return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training) 
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 7.66 
GiB of which 20.12 MiB is free. Including non-PyTorch memory, this process has 7.62 GiB memory in use. Of th
e allocated memory 7.47 GiB is allocated by PyTorch, and 24.71 MiB is reserved by PyTorch but unallocated. I
f reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to a
void fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.ht
ml#environment-variables) 
 1%|▍                                                                      | 2/318 [00:05<14:04,  2.67s/it] 
res@captiva ~/p/k/Q/AAUNaL (main) [1]>       

